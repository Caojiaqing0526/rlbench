{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "from toolz import pluck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import algos\n",
    "import features\n",
    "import parametric\n",
    "import policy\n",
    "import chicken\n",
    "from agents import OffPolicyAgent\n",
    "from rlbench import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_contextual(agent, env, max_steps, params=dict()):\n",
    "    ret = list()\n",
    "    \n",
    "    # reset the environment, get initial state, perform the run\n",
    "    t = 0\n",
    "    env.reset()\n",
    "    s = env.state\n",
    "    while not env.is_terminal() and t < max_steps:\n",
    "        actions = env.actions\n",
    "        a = agent.choose(s, actions)\n",
    "        r, sp = env.do(a)\n",
    "        \n",
    "        # update the agent\n",
    "        agent.update(s, a, r, sp, **params)\n",
    "        \n",
    "        # get the information from the agent\n",
    "        ret.append(agent.get_context(s, a, r, sp))\n",
    "    \n",
    "        # prepare for next timestep\n",
    "        t += 1\n",
    "        s = sp\n",
    "\n",
    "    # return the result\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Values\n",
    "\n",
    "The \"true\" values can be computed analytically in this case, so we did so.\n",
    "\n",
    "We can also compute the distribution for weighting the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True values:\n",
      "[ 0.4782969  0.531441   0.59049    0.6561     0.729      0.81       0.9        1.       ]\n",
      "On-policy distribution:\n",
      "[ 0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125]\n"
     ]
    }
   ],
   "source": [
    "num_states = 8\n",
    "gamma = 0.9\n",
    "true_values = gamma**np.arange(num_states)[::-1]\n",
    "d_pi = np.ones(num_states)/num_states\n",
    "D_pi = np.diag(d_pi)\n",
    "print(\"True values:\")\n",
    "print(true_values)\n",
    "print(\"On-policy distribution:\")\n",
    "print(d_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_value_dct(theta_lst, features):\n",
    "    return [{s: np.dot(theta, x) for s, x in features.items()} for theta in theta_lst]\n",
    "\n",
    "def compute_values(theta_lst, X):\n",
    "    return [np.dot(X, theta) for theta in theta_lst]\n",
    "\n",
    "def compute_errors(value_lst, error_func):\n",
    "    return [error_func(v) for v in value_lst]\n",
    "\n",
    "def rmse_factory(true_values, d=None):\n",
    "    true_values = np.ravel(true_values)\n",
    "    \n",
    "    # sensible default for weighting distribution\n",
    "    if d is None:\n",
    "        d = np.ones_like(true_values)\n",
    "    else:\n",
    "        d = np.ravel(d)\n",
    "        assert(len(d) == len(true_values))\n",
    "    \n",
    "    # the actual root-mean square error\n",
    "    def func(v):\n",
    "        diff = true_values - v\n",
    "        return np.sqrt(np.mean(d*diff**2))\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Errors\n",
    "\n",
    "For each algorithm, we get the associated experiment, and calculate the errors at each timestep, averaged over the runs performed with that algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDC\n",
      "TD\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8e5d6d519182>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0m_update_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alpha'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.002\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOffPolicyAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpol_pi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpol_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_update_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Run the experiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bab/git/git-research/rlbench/rlbench/algos.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;34m\"\"\" Make any changes necessary upon initialization.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bab/git/git-research/rlbench/rlbench/algos.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# TODO: Documentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bab/git/git-research/rlbench/rlbench/algos.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mold_rho\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "# define the experiment\n",
    "num_states = 8\n",
    "num_features = 6\n",
    "num_active = 3\n",
    "num_runs = 50\n",
    "max_steps = 5000\n",
    "\n",
    "\n",
    "# set up environment\n",
    "env = chicken.Chicken(num_states)\n",
    "\n",
    "# Define the target policy\n",
    "pol_pi = policy.FixedPolicy({s: {0: 1} for s in env.states})\n",
    "# Define the behavior policy\n",
    "pol_mu = policy.FixedPolicy({s: {0: 1} if s < 4 else {0: 0.5, 1: 0.5} for s in env.states})\n",
    "\n",
    "# state-dependent gamma\n",
    "gm_dct = {s: 0.9 for s in env.states}\n",
    "gm_dct[0] = 0\n",
    "gm_func = parametric.MapState(gm_dct)\n",
    "gm_p_func = parametric.MapNextState(gm_dct)\n",
    "\n",
    "# set up algorithm parameters\n",
    "update_params = {\n",
    "    'alpha': 0.03,\n",
    "    'beta': 0.002,\n",
    "    'gm': gm_func,\n",
    "    'gm_p': gm_p_func,\n",
    "    'lm': 0.0,\n",
    "    'lm_p': 0.0,\n",
    "    'interest': 1.0,\n",
    "}\n",
    "\n",
    "\n",
    "# Run all available algorithms \n",
    "data = dict()\n",
    "\n",
    "for name, alg in algos.algo_registry.items():  \n",
    "    print(name)\n",
    "    \n",
    "    run_lst = []\n",
    "    for i in range(num_runs):\n",
    "        print(\"Run: %d\"%i, end=\"\\r\")\n",
    "        episode_data = dict()\n",
    "        \n",
    "        # Want to use random features\n",
    "        phi = features.RandomBinary(num_features, num_active)\n",
    "        episode_data['features'] = {s: phi(s) for s in env.states}\n",
    "    \n",
    "        # Set up the agent\n",
    "        _update_params = update_params.copy()\n",
    "        if name == 'ETD':\n",
    "            _update_params['alpha'] = 0.002\n",
    "\n",
    "        agent = OffPolicyAgent(alg(phi.length), pol_pi, pol_mu, phi, _update_params)\n",
    "        \n",
    "        # Run the experiment\n",
    "        episode_data['steps'] = run_contextual(agent, env, max_steps)\n",
    "        \n",
    "        run_lst.append(episode_data)\n",
    "    data[name] = run_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline = rmse_factory(np.zeros(num_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# True values & associated stationary distribution\n",
    "theta_ls = np.array([ 0.4782969,  0.531441 , 0.59049, 0.6561, 0.729, 0.81, 0.9, 1.])\n",
    "d_pi = np.ones(num_states)/num_states\n",
    "D_pi = np.diag(d_pi)\n",
    "    \n",
    "# define the error/objective function\n",
    "err_func = rmse_factory(theta_ls, d=d_pi)\n",
    "baseline = err_func(np.zeros(num_states))\n",
    "\n",
    "for name, experiment in data.items():\n",
    "    print(name)\n",
    "    errors = []\n",
    "    for episode in experiment:\n",
    "        feats = experiment[0]['features']\n",
    "        X = np.array([feats[k] for k in sorted(feats.keys())])\n",
    "        steps = experiment[0]['steps']\n",
    "        thetas = list(pluck('theta', steps))\n",
    "\n",
    "        # compute the values at each step\n",
    "        val_lst = compute_values(thetas, X)\n",
    "        # compute the errors at each step\n",
    "        err_lst = compute_errors(val_lst, err_func)\n",
    "        errors.append(err_lst)\n",
    "    \n",
    "    # calculate the average error\n",
    "    clipped_errs = np.clip(errors, 0, 100) \n",
    "    avg_err = np.mean(clipped_errs, axis=0)\n",
    "    \n",
    "    # plot the errors \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(avg_err)\n",
    "    \n",
    "    \n",
    "    # format the graph\n",
    "    ax.set_ylim(1e-2, 2)\n",
    "    ax.axhline(baseline, c='red')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
